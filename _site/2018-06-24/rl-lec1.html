<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>
    
      Lecture 1: Introduction to Reinforcement Learning &middot; Deepsight Study Blog
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/assets/main.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i,700">

  <!-- Favicon -->
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon-16x16.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/apple-touch-icon.png">
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript" ></script>
</head>


  <body>
    <nav class="nav">
      <div class="nav-container">
        <div class="nav-logo">
          <img class="nav-img" src="/assets/logo.png" />  
        </div>
        <a href="/">
          <h2 class="nav-title">Deepsight Study Blog</h2>
        </a>
        <ul>
          <li><a href="/about">About</a></li>
          <li><a href="/">Posts</a></li>
        </ul>
      </div>
    </nav>

    <main>
      </head>
<div class="post">
  <div class="post-info">
    <span>Written by</span>
    
        Hyungkyu Kim
    

    
      <br>
      <span>on&nbsp;</span><time datetime="2018-06-24 00:00:00 +0900">June 24, 2018</time>
    
  </div>

  <h1 class="post-title">Lecture 1: Introduction to Reinforcement Learning</h1>
  <div class="post-line"></div>

  <h1 id="overview">Overview</h1>
<ul>
  <li><a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/intro_RL.pdf">slide</a></li>
  <li><a href="https://www.youtube.com/watch?v=2pWv7GOvuf0&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT">youtube</a></li>
</ul>

<hr />

<h1 id="contents">Contents</h1>
<p>“Reinforcement learning is learning what to do—how to map situations to actions—so as to maximize
a numerical reward signal”</p>

<p><img src="https://1.bp.blogspot.com/-1K7BtY7lwOk/WpomO1-FLmI/AAAAAAAAXLo/_EczW0T98HgdxcIwSSOchg6JFBQ2RgIswCLcBGAs/s640/4140_F1-5.PNG" alt="Alt text" />
RL에 필요한 여러 다른분야의 이야기가 많이 있는데 기본적으로 모든부분이 decision making을 어떻게 효과적으로 하는가에 대한 이야기이다. 좋은 의사결정은 상을 받고 그렇지 않다면 벌을 받는다</p>

<p>머신러닝은 크게 다음과 같은 3가지 카테고리로 분류가능
<img src="http://solarisailab.com/wp-content/uploads/2017/06/supervsied_unsupervised_reinforcement.jpg" alt="Alt test" /></p>

<p>강화학습은 학습을 하는 “방식”으로 구분되는 것이 아니고 강화학습 “문제”인가로 구분된다.</p>

<p>강화학습의 특징은 다음과 같다</p>
<ul>
  <li>supervisor 가 없다 (어떤게 최선인지 알려주지 않는다. 오로지 Trial and error를 통해)</li>
  <li>feed back이 delay되어 들어옴</li>
  <li>시퀀셜한 데이터가 들어옴(not i.i.d data -&gt; 각 데이터가 correlation 되어 있음)</li>
  <li>agent의 action에 따라 input data에 영향이 생긴다</li>
</ul>

<h2 id="architecture">architecture</h2>
<p>전체 구조는 다음과 같다. 여기서 목적은 뇌 그림쪽에 있는 agent를 어떤식으로 만들것인가다. 
<img src="https://doc-0o-84-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/do0217mrp5vvvbmcotk4gc6m79v3pj27/1529834400000/02854589234465558495/*/1qZfQVVk0aYbNnmM8kenZDEk6dQzOHQVi" alt="Alt text" /></p>
<ul>
  <li>뇌: agent</li>
  <li>
    <p>지구: environment</p>
  </li>
  <li>At each step t the agent:
    <ul>
      <li>Executes action $A_t$</li>
      <li>Receives observation $O_t$</li>
      <li>Receives scalar reward $R_t$</li>
    </ul>
  </li>
  <li>The environment:
    <ul>
      <li>Receives action $A_t$</li>
      <li>Emits observation $O_{t+1}$</li>
      <li>Emits scalar reward $R_{t+1}$</li>
    </ul>
  </li>
  <li>t increments at env. step</li>
</ul>

<p>agent입장에 action만이 environment 에 영향을 미치는 채널이다. agent에 input, output되는 모든것들이<a href="##history_and_state">history, experience</a>를 형성</p>

<p>전체 스토리를 상기하자. 시작은 RL의 Goal 부터</p>
<ol>
  <li>RL의 Goal은 미래의 예상되는 <a href="##Reward">Reward</a>합을 최대화 시키는 것. 최대화 할 수 있는 path를 따라야한다</li>
  <li>agent는 그 path를 시작하는 action을 pick해야한다</li>
  <li>agent에서 action을 pick하기 위해서는 state필요</li>
  <li>미래의 action을 pick해야 하니까 미래의 state를 예측해야 한다. <a href="##observation">observation</a>에 따라 다음 두가지 경우
    <ul>
      <li>Fully Observable: $state_t+1$은 $state_t$(history)를 MDP에 넣어 생성
    * Partial Observable: Beliefs또는 rnn 통해 생성</li>
    </ul>
  </li>
</ol>

<h2 id="reward">Reward</h2>
<p>Reinforcement learning is based on the <strong>reward hypothesis</strong>. RL은 이 reward를 최대로 쌓는 쪽으로 움직인다.</p>

<p>Reward의 특징은 다음과 같다</p>
<ul>
  <li>A reward Rt is a scalar feedback signal</li>
  <li>Indicates how well agent is doing at step t</li>
  <li>The agent’s job is to maximise cumulative reward</li>
</ul>

<p>single scalar feedback은 효과적. 의사결정 시 여러 goal 중 어떤것을 따를 것인가를 보는데, 이것은 RL관점에서는 어떤 action을 pick 할 것이냐라는 말. 해서 같은 축(thresh hold 같은)에 놓고 비교를 해야하는데 이때 scalar가 효과적</p>

<p>Sequential Decision Making 이야기로 가면</p>
<ul>
  <li>Goal: select actions to maximise total future reward</li>
  <li>Actions may have long term consequences</li>
  <li>Reward may be delayed</li>
  <li>It may be better to sacrifice immediate reward to gain more long-term reward</li>
</ul>

<h2 id="history-and-state">History and State</h2>
<p>The <strong>history</strong> is the sequence of observations, actions, rewards
<script type="math/tex">H_t = O_1, R_1, A_1, ... ,A_t, O_t, R_t</script>
여기서 state가 사용되는게 아니고 ot가 사용되는 이유는 agent는 env중에 관측가능한 부분만 받아들일 수 있기때문.
히스토리는 experience of the agent. 이는 RL에서 사용되는 데이터다(stream of data)</p>

<p>What happens next depends on the history:</p>
<ul>
  <li>The agent selects actions</li>
  <li>The environment selects observations/rewards</li>
</ul>

<p><strong>State</strong> is the information used to determine what happens next. 다음에 뭐가 일어날지 결정하는데 사용할 정보(history)의 summary. State는 설정하기 나름 영상이라고 해서 반드시 현재의 1프레임일 필요없다. deep q net 논문에서는 Atari game에 4프레임을 합쳐서 사용</p>

<p>Formally, state is a function of the history:
<script type="math/tex">S_t = f (H_t )</script>
보통은 valid definition state 는 last 샘플만, 그전꺼는 무시 (MDP). 일반적으로 RL에서 state라고하면 agent state를 이야기함</p>

<p>Information State는 state를 수학적으로 표현하는 방법. <strong>Markov state</strong>라고 한다. 
<script type="math/tex">\mathbb{P}[S_{t+1} | S_t] = \mathbb{P}[S_{t+1} | S_1,...,S_t]</script>
위 수식을 만족할때 Markov 하다고 함. 다음 스테이트를 생성할때 현재의 스테이트를 통해 생성될 확률과 현재에서부터 모든 과거를 포함한 스테이트들을 통해 생성되는 확률이 같을때. 조금더 풀어쓰면 다음과 같다.
<script type="math/tex">H_{1:t} -> S_t -> H_{t+1:\infty}</script>
위에서 기술했듯이 현재상태는 히스토리에 펑션을 돌려서 나온 결과, 이를 통해 미래 모든 히스토리를 예측가능. 미래의 모든 히스토리에는 미래의 모든 리워드들도 포함되어 있기때문에 리워드들을 예측가능함.</p>

<p>Environment state는 Markov 하다. 또한 history는 Markov 하다</p>

<h3 id="1-environment-state">1. Environment State</h3>
<p>진짜 환경의 상태. action을 인풋으로 받고 현재의 환경의 상태를 스까서 다음 observation, reward를 생성. agent에서 전체가 안보일 수 있음. agent에서 관측가능하더라도 사용못할 정보들이 있을 수 있음</p>

<h3 id="2-agent-state">2. Agent State</h3>
<p>에이전트 내부에서 관리하는 상태. observation을 갖고 만P든다
에이전트 스테이트는 알고리즘안에 현재 환경이 어떤지(observation)를 summarize하고 다음 액션을 픽하는데 사용.  observation에서 어떤 정보를 취득하고 버릴것인가. pick action!!!!</p>

<p>It can be any function of history
<script type="math/tex">S^a_t = f(H_t )</script></p>

<h2 id="observation">observation</h2>
<p>환경에서 일어나는 일들을 관측.</p>

<h3 id="1-fully-observable-environments">1. Fully Observable Environments</h3>
<p>Full observability: agent directly observes environment state
<script type="math/tex">O_t = S^a_t = S^e_t</script></p>

<p>environment는 markov 하므로 agent state또한 markov하다. 따라서 다음 스테이트는 MDP로 생성</p>

<p>문제사이즈가 매우 작거나, 말도안되는 연산능력과 저장용량이 있거나 일때 가능할듯</p>

<h3 id="2-partially-observable-environments">2. Partially Observable Environments</h3>
<p>Partial observability: agent indirectly observes environment</p>

<p>보통 일반적인 상황. 환경의 일부만을 관측한다. 다음 스테이트는 partially observable Markov decision process
(POMDP)로</p>

<p>Beliefs 해당 status가 일어날 확률로 표현. 이 벡터들을 전부 더해서 스테이트로 쓴다.
<script type="math/tex">S^a_t = (\mathbb{P}[S^e_t = s^1], ... , \mathbb{P}[S^e_t = s^n])</script>
RNN 스테이트를 리니어하게 표현
<script type="math/tex">S^a_t = \sigma(S^a_{t-1}W_s + O_tW_o)</script></p>

<h2 id="major-components-of-an-rl-agent">Major Components of an RL Agent</h2>
<p><img src="https://doc-0s-84-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hta2re7efb0dsciotiecjn7d9much3n5/1529834400000/02854589234465558495/*/19jlmgeiHGNuhGevJQoStLREUnZ1d0KjD" alt="Alt text" /></p>
<ul>
  <li>policy: 다음 액션을 어떻게 pick 할 것인가. agent의 행동양식</li>
  <li>value function: 특정 state나 특정 action에 가치를 부여. action value function은 q value func라고함</li>
  <li>model: agent가 env를 보는 view.
    <h3 id="1-policy">1. Policy</h3>
    <p><img src="https://doc-0k-84-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/58pnb2rpieb7fseuu2a9sprfe4j5agmq/1529834400000/02854589234465558495/*/1YIyZH1t18WQoWVQasX1ldU1PWnpIGjRS" alt="Alt text" /></p>
  </li>
  <li>policy is the agent’s behaviour</li>
  <li>It is a map from state to action, e.g.</li>
  <li>Deterministic policy: a = <script type="math/tex">\pi(s)</script></li>
  <li>Stochastic policy: $\pi(a|s) = \mathbb{P}[A_t = a|S_t = s]$
    <h3 id="2-value-function">2. Value function</h3>
    <p><img src="https://doc-0g-84-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/0jeam7dvm2s77o2o70bt3gi3f4nu6tor/1529834400000/02854589234465558495/*/1j_Hh_BPlo5QRfGxjuGEp5yELg5BKFufo" alt="Alt text" /></p>
  </li>
  <li>Value function is a prediction of future reward</li>
  <li>Used to evaluate the goodness/badness of states</li>
  <li>And therefore to select between actions, e.g.
    <ul>
      <li>$\gamma$: discount 비율
<script type="math/tex">v_{\pi}(s) = E_{\pi}[R_t+1 + {\gamma}R_{t+2} + \gamma^2R_{t+3} + ... | S_t = s]</script></li>
    </ul>
  </li>
</ul>

<h3 id="3-model">3. Model</h3>
<p><img src="https://doc-00-84-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/d4ijquh634e469dr9vvvobpucsoqrc7e/1529834400000/02854589234465558495/*/1IDl-9ApvqmUJLzLKDak1KTbuwqxe3_El" alt="Alt text" /></p>

<h2 id="categorizing-rl-agents">Categorizing RL agents</h2>
<p><img src="https://i1.wp.com/www.yittoo.com/blog/wp-content/uploads/2017/03/rl-agent-taxonomy-e1490514515977.jpg?resize=372%2C291" alt="Alt text" /></p>
<ul>
  <li>Value Based
    <ul>
      <li>No Policy (Implicit)</li>
      <li>Value Function</li>
    </ul>
  </li>
  <li>Policy Based
    <ul>
      <li>Policy</li>
      <li>No Value Function</li>
    </ul>
  </li>
  <li>Actor Critic
    <ul>
      <li>Policy</li>
      <li>Value Function</li>
    </ul>
  </li>
  <li>Model Free
    <ul>
      <li>Policy and/or Value Function</li>
      <li>No Model</li>
    </ul>
  </li>
  <li>Model Based
    <ul>
      <li>Policy and/or Value Function</li>
      <li>Model</li>
    </ul>
  </li>
</ul>

<h2 id="exploration-and-exploitation">Exploration and Exploitation</h2>
<ul>
  <li>Exploration nds more information about the environment</li>
  <li>Exploitation exploits known information to maximise reward</li>
  <li>It is usually important to explore as well as exploit
    <h2 id="video">video</h2>
    <p>13p
single scalar feedback은 효과적. 
여러 복잡한 goal 중 어떤것을 따를 것인가, optimizing 할것인가. 
RL 관점에서는 pick action이 goal로 가는 방법. 
그 goal들을 비교하기 위해 같은 축(thresh hold 같은)에 놓고 비교를 해야하는데 이때 scalar가 효과적</p>
  </li>
</ul>

<p>16p 
우리는 이 뇌 안쪽에 알고리즘, agent 를 만드는게 목표다</p>

<p>17p 
env는 obs 와 reward를 gen한다.
env는 어쩔수 없다. action만이 env에 영향을 미치는 채널이다.
that time series defines the experience of the agent
experience는 RL에서 사용되는 데이터다(stream of data)</p>

<p>18p
State 중요한 개념. summary 다음에 뭐가 일어날지 결정하는데 사용할 정보.
valid definition state 는 last 샘플만, 그전꺼는 무시</p>

<p>19p 
질문 멀티 에이전트에서는 어케? 다른 에이전트들은 그냥 환경으로 생각. 에이전트 브레인에서 환경속 브레인으로 전달가능? 가능하나 …</p>

<p>20p 
에이전트 스테이트는 알고리즘안에 현재 환경이 어떤지(obs)를 summarize하고 다음 액션을 픽하는데 사용. our desicion은 observation에서 어떤 정보를 취득하고 버릴것인가. pick action!!!!</p>

<p>21p 
information stated은 Markov state라고도 불리며 기본적으로 information 이론적인 컨셉이다
이것은 언제 state를 만드는 것이 hitory 의 유용한 정보들을 모두 포함할 것인가를 설명
미래의 h를 예측하기위해 모든 과거의 h가 필요없음. 단시 현재의 s만이 필요
S_t는 미래의 모든 history생성하는데 사용됨. history는 r을 포함하니까 미래의 모든 r을 구할 수 있다. 
마르코프는 과거는 필요없음 미래를 예측하는데 현재의 상태만이 필요하다. 
질문 헬기 이야기. 스테이트는 각도 위치 속도 각속도 등등, 10분전 상태 같은 예전 궤도는 필요없다
non 마코프에서는 필요함
마코프에서 $ H_t+1\infty $ 는 미래의 모든리워드 포함</p>

<p>p23
세개 모두 같은량, env stat이 agent stat으로 사용됨. best case. state를 표현하는 form MDP. 모두 환경요소를 관측이 불가능하기때문에 MDP를 이용해서 실제적인 문제를 이해</p>

<p>p24
파샬관측. 
방안로봇은 카메라를 통해 방안만을 본다. 
트레이딩 에이전트. 현재 가격만 본다. 어디서 왔는지 모름
카드. 테이블에 까진것만 안다. 숨겨진거 모름
Beliefs 해당 env stat이 일어날 확률로 표현. 이 벡터들을 전부 더해서 스테이트로 쓴다
RNN 스테이트를 리니어하게 표현</p>

<p>p25
policy: 다음 액션을 어떻게 pick 할 것인가
value: state와 action에 가치를 부여
modle: agent가 env를 보는 view</p>

<p>p26
deterministic: 매핑함. 경험을 통해 배움. 리워드를</p>

<p>p31 
리스크: ???</p>

<p>p33
transition: 헬기가 이런 각도와 이런 방향 이런 위치에 있을떄 바람이 불면 이렇게 뒤집는다. 어떤일이 다음스텝에 일어날지 이거를 배운다
Rewards: 액션 픽할떄 리워드
옵셔널 ??? 많은 코스에서 모델 프리 메서드에 주목한다</p>

<p>p33
에이전트가 위쪽 궤도를 따라갔다고 했을때 
매 스텝마다 리워드가 -1 되는것을 나타낸다. 타임 지날때마다 -1이라고 써있음. 이해된 다이나믹 env를 나타냄</p>

<p>p34
value base: policy 안쓰고 리워드 좋은놈을 쫓아간다</p>

<p>p35
model free: 명확하게 환경을 이해하려고 하지 않음. 헬기 예제 헬기 움직임. 단순히 바로 exprience에 p , v
model base: 환경이 어케변할지 모델 만ㄷ름</p>

<p>p37
reinforceme l: 환경안다.
planning: 다르다
작업을 할 환경에 대한 모든것을 알려줌. 게임의 룰. 헬기라면 바람에대한 수식들같은…
실제 환경에서 수행하는게 아닌 완벽한 인터널 모델과 인터액션으로 계산을 수행
그렇게 policy를 업그레이드</p>

<p>p38
실제로 게임 하는것 RL</p>

<p>p39
에뮬레이터로.
오른쪽 그림과같이 액션에 따라서 반응하는 완벽한 환경이 갖춰져있음. 액션에따른 환경변화의 서브트리를 만들 수 있다.</p>

<p>p41
exploration은 리워드를 어느정도 포기하고 env에서 새로운것을 찾는다
exploitation은 리워드를 최대치로하는 쪽으로</p>

<p>p43 ???
Prediction: 현재의 policy를 따랐을때 agent가 할 행동이 얼마나 좋을까
control: policy를 최적화
RL에서는 이 2문제를 모두 풀어야함</p>

<p>Q: MKP 에서 과거 스텟을 신경 안쓴다면 아타리게임서 4프레임 합쳐서 하는건 뭐냐
A: 걍보는 관점</p>

<p>Q: 언제 디터미니, 스토케스틱
A: 도달하는 곳이 결정적이면 디터, 여러개면 스토</p>

<p>Q: value based가 deep q n, plocy 는 po..?
A: q는 action value function</p>


</div>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript" ></script>

<div class="pagination">
  
  
    <a href="/2018-06-16/join-deepsight-study-member" class="right arrow">&#8594;</a>
  

  <a href="#" class="top">Top</a>
</div>



    </main>

    <footer>
      <span>
          &copy; <time datetime="2018-06-16 14:58:11 +0900">2018</time> <a href="https://github.com/deepsight/">Deepsight</a>.
      </span>
    </footer>
  </body>
</html>
